\documentclass[handout]{beamer}
\usetheme[titleprogressbar]{m}

%\usepackage{pgfpages}
%%\setbeameroption{show notes}
%\setbeameroption{show notes on second screen=right}

\setbeamertemplate{caption}[numbered]

\usepackage{tikz}
\usetikzlibrary{calc, matrix, arrows, arrows.meta, automata, shapes, positioning, shadows, trees}

\definecolor{myyellow}{RGB}{240,217,1}
\definecolor{mygreen}{RGB}{143,188,103}
\definecolor{myred}{RGB}{234,38,40}
\definecolor{myblue}{RGB}{53,101,167}

\usepackage{graphicx}
\usepackage{multimedia}
%\usepackage[latin1]{inputenc}
\setbeamercovered{transparent}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{color}
\usepackage{subfigure}

\tikzset{basic/.style={draw}}
\tikzset{input/.style={basic, circle}}
\tikzset{bias/.style={basic, rectangle}}
\tikzset{neuron/.style={basic, circle}}
\tikzset{>=stealth, font=\scriptsize}
\tikzset{sectors/.style n args={2}{%
     circle, draw, minimum width=3.444em,
     append after command={%
         \pgfextra{ %
            \draw (\tikzlastnode.north) -- (\tikzlastnode.south);
            \path (\tikzlastnode.center) -- node[] {#1} (\tikzlastnode.west);
            \path (\tikzlastnode.center) -- node[] {#2} (\tikzlastnode.east);
         }
      }
   }
}

% Beamer stuff
\newcommand\ppbb{path picture bounding box}
\setbeamerfont{bibliography item}{size=\tiny}
\setbeamerfont{bibliography entry author}{size=\tiny}
\setbeamerfont{bibliography entry title}{size=\tiny}
\setbeamerfont{bibliography entry location}{size=\tiny}
\setbeamerfont{bibliography entry note}{size=\tiny}


% argmin
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{min}}\;}
% argmax
\newcommand{\argmax}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{max}}\;}

\title[\insertdate]{From Linear Algebra\\to Machine Learning}

\author{Omar Guti\'errez}
\institute{@trinogz}
\date{\today}

\begin{document}

\maketitle

\begin{frame}
    \frametitle{Overview}
    \tableofcontents
\end{frame}


\section{Motivation}
\begin{frame}{Motivation}
    \begin{itemize}
        \item \textbf{Linear algebra} is important to understand machine learning.
        \item As well as \textbf{calculus}, \textbf{probability theory}, and \textbf{statistics}.
        \item It is rewarding to take the \textbf{hard path} to learn machine learning (IMHO).
    \end{itemize}
\end{frame}


\begin{frame}{Learning from errors}
    \includegraphics[scale=0.555]{figures/naive_distance_matrix_r.png}
\end{frame}


\begin{frame}{Learning from errors}
    \includegraphics[scale=0.555]{figures/naive_euclidean_r.png}
\end{frame}


%    columns
%    \begin{columns}
%        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%        \begin{column}{0.5\textwidth}
%        \end{column}
%
%        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%        \begin{column}{0.5\textwidth}
%        \end{column}
%
%    \end{columns}


\section{Vectors}
%\subsection{Length of a vector}
\begin{frame}{A vector is a collection of numbers}
    \Huge
    \begin{align*} 
        \textcolor{blue}{\vec{a} = \boldsymbol{a}} &= \begin{bmatrix}
           a_{1}  \\
           a_{2}  \\
           \vdots \\
           a_{n}  
         \end{bmatrix}
    \end{align*}
\end{frame}


\begin{frame}{Length of a vector}
    \huge
    \begin{align*}
        \textcolor{blue}{|\boldsymbol{a}|}   &= \sqrt{\sum^{n}_{i=1} a_i^2}
        %\textcolor{blue}{
        %||\boldsymbol{x}||_p = \Bigg(\sum^{n}_{i=1} |x_i|^p\Bigg)^\frac{1}{p}
        %}
    \end{align*}
\end{frame}
\note{
    \begin{itemize}
        \item remark that sometimes vectors have a symbol above, or are written in bold letters
    \end{itemize}
}

%\subsection{Distance}
\begin{frame}{Distance beetwen vectors}
    \Large
    \begin{align*}
        d(\boldsymbol{a}, \boldsymbol{b}) &= ||\boldsymbol{a} - \boldsymbol{b}||\\
                                          &= \sqrt{\sum_{i=0}^n (a_{i} - b_{i})^2}
    \end{align*}
\end{frame}


%\subsection{Dot product}
\begin{frame}{Dot product}
    \Large
    \begin{align*}
        \textcolor{red}{a \cdot b} &= \textcolor{brown}{\sum_{i=0}^{n} a_i b_i} \\
                                   &= \textcolor{blue}{a_0b_0 + a_1b_1 + \ldots + a_nb_n}
    \end{align*}
    So,
    \begin{align*}
        \textcolor{red}{a \cdot a} &= a_0a_0 + a_1a_1 + \ldots + a_na_n \\
                                   &= a_0^2  + a_1^2  + \ldots + a_n^2 \\
                                   &= |\boldsymbol{a}|^2
    \end{align*}
\end{frame}

\section{Examples}
%\subsection{Perceptron}
{
\usebackgroundtemplate{\includegraphics[width=\paperwidth, height=\paperheight]{figures/winter-is-coming.jpg}}%
\begin{frame}{}
\end{frame}
}

\begin{frame}{The AI winter is coming}
    \begin{itemize}
        \item Is really coming? No.
        \item However, we already had an AI winter.
        \item The research on neural nets was stopped for many years, after
                Minsky and Papert proved that a single layer perceptron was not able to
                deal with the exclusive-or problem.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Regression}
    \begin{figure}[htb]
        \centering
        \input{diagrams/regression.tex}
        \label{fig:regression}
	\end{figure}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Linear Regression}
    \Large
    \begin{itemize}
        \item We want to calculate the intercept $a$ and the slope $b$.
    \end{itemize}
    \begin{align*}
        \argmin{a, b} \sum_i (y_i - (ax_i + b))^2 =
        \argmin{\boldsymbol{w}} || \boldsymbol{Xw} - \boldsymbol{y} ||^{2}
    \end{align*}
    \begin{itemize}
        \item The solution to this optimization problem is:
    \end{itemize}
    \begin{align*}
        \boldsymbol{w^{*}} = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^{T}\boldsymbol{y}\,.
    \end{align*}
\end{frame}

%\begin{frame}[fragile]\frametitle{Toy dataset}
%    \begin{figure}[htb]
%        \centering
%        \input{diagrams/dataset.tex}
%        \label{fig:toy}
%	\end{figure}
%\end{frame}
%
%\begin{frame}[fragile]\frametitle{What is happening?}
%    \begin{figure}[htb]
%		\centering
%		\input{diagrams/dot_product.tex}
%		\label{fig:issue}
%    \end{figure}
%\end{frame}

\section{Conclusions}
\begin{frame}{References}
    \begin{itemize}
        \item \textbf{Mathematics for Machine Learning: Linear Algebra} offered by Coursera.
        \item \textbf{The Math of Intelligence} offered by Siraj Raval.
    \end{itemize}
\end{frame}

\begin{frame}
\huge{Thank you.}\\
\huge{Questions?}\\
\huge{Comments?}\\
\end{frame}

\end{document}
